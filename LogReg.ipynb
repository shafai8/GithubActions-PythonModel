{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('./pullreqs.csv')\n",
    "\n",
    "\n",
    "# Apply text normalization to 'Description' column\n",
    "df['Description'] = df['Description'].apply(normalize_text)\n",
    "\n",
    "# shuffle the dataset\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Preprocessing steps\n",
    "# Split data into features (X) and target variable (y)\n",
    "X = df['Description']\n",
    "y = df['Class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "# Create pipeline with TF-IDF vectorizer and logistic regression classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Load validation dataset\n",
    "df_validation = pd.read_csv('./internal.csv')\n",
    "\n",
    "# Apply the same text normalization to 'Description' column\n",
    "df_validation['Description'] = df_validation['Description'].apply(normalize_text)\n",
    "\n",
    "# Preprocessing steps for validation data\n",
    "X_val = df_validation['Description']\n",
    "y_val = df_validation['Class']\n",
    "\n",
    "# Make predictions on validation data\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "\n",
    "# Convert y_val to string data type\n",
    "y_val = y_val.astype(str)\n",
    "\n",
    "# Evaluation on validation data\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization function\n",
    "def normalize_text(text):\n",
    "    \n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into text\n",
    "    normalized_text = ' '.join(tokens)\n",
    "    \n",
    "    return normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using joblib\n",
    "joblib.dump(pipeline, 'my_model.pkl')  # Replace with your desired filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class for the new text: Merged\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model using joblib\n",
    "loaded_pipeline = joblib.load('my_model.pkl')  \n",
    "\n",
    "# Prediction on new text sample\n",
    "txt = \"Issue number: N/A What is the current behavior? As a takeaway from our learning session about a menuController bug in Ionic Angular, the team would like to update our other providers to use the same architecture as the menuController to prevent this kind of issue from happening again in the future. We also noticed that the common provider does not provide much value and it's easier to just have two separate implementations in src and standalone. (There wasn't much code we could de-duplicate) What is the new behavior? Removed the common modal provider in favor of separate ones in src/standalone We already have test coverage for the modalController, so I did not add new ones Does this introduce a breaking change? Yes No Other information.\" \n",
    "\n",
    "# Preprocess the new text following the same normalization steps\n",
    "new_text = normalize_text(txt)\n",
    "\n",
    "# Convert the new text into a list (single sample)\n",
    "new_text_list = [new_text]\n",
    "\n",
    "# Use the loaded pipeline to predict the class label\n",
    "prediction = loaded_pipeline.predict(new_text_list)[0]\n",
    "\n",
    "# Print the predicted class\n",
    "print(f\"Predicted class for the new text: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize_text(\"I am a student, and I am studying at the University of Jordan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
